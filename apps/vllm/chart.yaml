apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: vllm-chart
  namespace: argocd
spec:
  project: default
  source:
    chart: vllm-stack
    repoURL: https://vllm-project.github.io/production-stack
    targetRevision: 0.1.8
    helm:
      values: |
        servingEngineSpec:
          runtimeClassName: ""
          modelSpec:
          - name: "llama3"
            repository: "vllm/vllm-openai"
            tag: "latest"
            modelURL: "meta-llama/Llama-3.2-1B-Instruct"

            replicaCount: 1

            requestCPU: 6
            requestMemory: "16Gi"

            pvcStorage: "50Gi"
            pvcAccessMode:
              - ReadWriteOnce

            hf_token:
              secretName: "vllm-hf-token"
              secretKey: "token"

            env:
              - name: OTEL_SERVICE_NAME
                value: "vllm-llama3"
              - name: OTEL_EXPORTER_OTLP_ENDPOINT
                value: "http://otel-collector-opentelemetry-collector.otel-collector.svc.cluster.local:4317"
              - name: OTEL_RESOURCE_ATTRIBUTES
                value: "service.name=vllm-llama3,deployment.environment=homelab"

  destination:
    server: https://kubernetes.default.svc
    namespace: vllm
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
